#!/usr/bin/env python3

from __future__ import division

import sys, os, time, argparse, shutil, hashlib, math
import numpy as np
from operator import itemgetter

from snps_io import id_genome_clusters, id_centroid
from snps_io import vcf_io, gen_msa, align_assembly

def get_data_type():
	""" Get program specified by user (species, genes, or snps) """
	import sys
	if len(sys.argv) == 1 or sys.argv[1] in ['-h', '--help']:
		cmd = '%s ' % (os.path.basename(sys.argv[0]))
		print('usage: %s <data_type> [options]' % cmd)
		print('')
		print('description: identify core-genome snps from <data_type>')
		print('')
		print('data types:')
		print('	genomes			 perform multiple alignment of genomes to call SNPs (default)')
		print('	msa				 use input multiple genome alignment to call SNPs')
		print('')
		print("use '%s <data_type> -h' for usage on a specific command" % cmd)
		print('')
		quit()
	elif sys.argv[1] not in ['genomes', 'msa']:
		sys.exit("\nError: invalid data type\n")
	else:
		return sys.argv[1]

def parse_args():

	data_type = get_data_type()

	parser = argparse.ArgumentParser(
		formatter_class=argparse.RawTextHelpFormatter,
		add_help=False,
		usage=argparse.SUPPRESS
		)

	#parser.add_argument('program', help=argparse.SUPPRESS)
	parser.add_argument('data_type', help=argparse.SUPPRESS)

	io = parser.add_argument_group('input/output')
	if data_type in ['genomes', 'genomes-alt']:
		io.add_argument('--fna-dir', type=str, metavar='PATH', required=True,
			help = """Path to directory of genomes in FASTA format""")
	elif data_type == 'msa':
		io.add_argument('--msa-path', type=str, metavar='PATH', required=True,
			help = """Path to multiple-genome alignment""")
		io.add_argument('--msa-type', choices=['xmfa-parsnp', 'xmfa-mummer4'], required=True,
			help = """File format of multiple-genome alignment""")

	if data_type in ['genomes']:
		io.add_argument('--subset-list', type=str, metavar='PATH', default=None, help = """Path to file contains the names of the fullset or subset of the files in the input directory. Files not in the list will not be included for snp calling""")

	if data_type in ['genomes', 'msa']:
		io.add_argument('--rep-fna', type=str, metavar='PATH', default=None,
			help = """Path to the reference genome serving as the template for whole genome alignment. If provided, CallM will not identify and use centroid genome for reference""")
		io.add_argument('--has-completeness', action='store_true', default=False,
			help = """Toggle for specifying completeness for supplied genomes sequenes. If toggled on, it requries to supply either --completeness or --completeness-list""")
		io.add_argument('--completeness', type=float, metavar='FLOAT', default=None,
			help = """Single completeness value for all genomes sequenes (i.e. all genomes have the same completeness)""")
		io.add_argument('--completeness-list', type=str, metavar='PATH', default=None,
			help = """Path to list of pairs of genome file name and completeness value, separated by tab character. (note: genome file names should have no duplicates, and should match and cover all files in the directory speficied by --fna-dir)""")
		io.add_argument('--missing-ratio', type=float, metavar='FLOAT', default=0.05,
			help = """Parameter defining the missing ratio of core sites even when completeness is 1""")
		io.add_argument('--min-pid', type=float, metavar='FLOAT', default=0,
			help = """Parameter defining the minimal identity for including each aligned block, [0, 100]""")
		io.add_argument('--min-aln-len', type=int, metavar='INT', default=10,
			help = """Parameter defining the minimal length for including each aligned block""")
		io.add_argument('--max-pid-delta', type=float, metavar='FLOAT', default=0.1,
			help = """Parameter defining the maximum identity gap between identity of each aligned block and whole-genome ANI, all alignments with identity less than ANI * (1 - delta) will be purged, [0, 1]""")

	if data_type in ['genomes']:
		io.add_argument('--skip-align', action='store_true', default=False,
			help = """skip whole genome sequence or short read alignment, only applicable when alignment has already been done""")

	if data_type in ['genomes']:
		prep = parser.add_argument_group('preprocessing')
		
		prep.add_argument('--keep-redundancy', action='store_true', default=False,
			help="""If toggled on, CallM will skip redundancy removal and move on with all input genomes""")
		prep.add_argument('--skip-centroid', action='store_true', default=False,
			help="""If toggled on, CallM will not attempt to identify and use centroid genome for reference""")
		prep.add_argument('--sketch-k', type=int, metavar='INT', default=21,
			help="""k-mer size for building Mash sketch""")
		prep.add_argument('--sketch-size', type=int, metavar='INT', default=5000,
			help="""The number of k-mers per Mash sketch""")
		prep.add_argument('--precut', type=float, metavar='FLOAT', default=0.02,
			help="""Limit searches among pair of genomes with distance smaller than the provided value""")
		prep.add_argument('--start-cutoff', type=float, metavar='FLOAT', default=0.02,
			help="""The cutoff from which CallM will start to search a distance cutoff, which generate the good number of genome clusters and tag genomes based on a given MAF""")
		prep.add_argument('--end-cutoff', type=float, metavar='FLOAT', default=0.0001,
			help="""Similiar to --start-cutoff, the cutoff at which CallM will end the search for a distance cutoff. This value should be smaller than --start-cutoff""")
		prep.add_argument('--range-factor', type=float, metavar='FLOAT', default=1.2,
			help="""This factor times the minimum number of genomes needed for a given MAF will create the upper bound of a range satisfying the search. It should be larger than 1.""")

	io.add_argument('--iter', action='store_true', default=False,
		help = """calling SNPs iteratively, option for memory saving""")

	io.add_argument('--out-dir', type=str, metavar='PATH', required=True,
		help="""Directory to store output""")

	snps = parser.add_argument_group('snp-calling')
	snps.add_argument('--max-sites', type=int, metavar='INT', default=float('inf'),
		help="""Maximum genomic sites to parse (use all); useful for testing""")
	snps.add_argument('--min-prev', type=float, metavar='FLOAT', default=1.0,
		help="""Minimum prevalence (1.0)""")
	snps.add_argument('--snp-freq', type=float, metavar='FLOAT', default=0.01,
		help="""Minimum minor allele frequency for SNP calling (0.01)""")
	snps.add_argument('--max-samples', type=int, metavar='INT', default=float('inf'),
		help="""Only use a subset of genomes or metagenomes for snp calling""")
	if data_type == 'reads':
		snps.add_argument('--sample-depth', type=float, metavar='FLOAT', default=5.0,
			help="""Minimum depth for including a sample (5.0)""")
		snps.add_argument('--site-depth', type=int, metavar='INT', default=1,
			help="""Minimum depth for genotyping a site in a sample (1)""")

	misc = parser.add_argument_group('misc')
	misc.add_argument("-h", "--help", action="help",
		help="""Show this help message and exit""")
	misc.add_argument('--threads', type=int, metavar='INT', default=1,
		help="""Number of CPUs to use (1)""")

	args = vars(parser.parse_args())

	return args

def run_command(cmd, env=None):
	import subprocess as sp
	if env:
		p = sp.Popen(cmd, shell=True, stdout=sp.PIPE, stderr=sp.PIPE, env=env)
	else:
		p = sp.Popen(cmd, shell=True, stdout=sp.PIPE, stderr=sp.PIPE)
	out, err = p.communicate()
	if p.returncode != 0:
		err_msg =  "\nError: the following returned non-zero status: '%s':\n" % cmd
		err_msg += "\n%s" % err
		sys.exit(err_msg)
	else:
		return out, err

def parallel(function, argument_list, threads):
	""" Based on: https://gist.github.com/admackin/003dd646e5fadee8b8d6 """
	import multiprocessing as mp
	import signal
	import time

	def init_worker():
		signal.signal(signal.SIGINT, signal.SIG_IGN)

	pool = mp.Pool(int(threads), init_worker)

	try:
		results = []
		for arguments in argument_list:
			p = pool.apply_async(function, args=arguments)
			results.append(p)
		pool.close()

		while True:
			if all(r.ready() for r in results):
				return [r.get() for r in results]
			time.sleep(1)

	except KeyboardInterrupt:
		pool.terminate()
		pool.join()
		sys.exit("\nKeyboardInterrupt")

def reformat_sequence_headers(args):
	"""
	Reformat sequence headers in input genomes to prevent parsnp from crashing
	"""
	import Bio.SeqIO
	if 'fna_dir' in args:
		try: os.makedirs(args['out_dir']+'/temp/genomes')
		except: pass
		for file in os.listdir(args['fna_dir']):
			infile = open(args['fna_dir']+'/'+file)
			outfile = open(args['out_dir']+'/temp/genomes/'+file, 'w')
			for seq in Bio.SeqIO.parse(infile, 'fasta'):
				seq.id = seq.id.replace('-', '_')
				seq.seq = str(seq.seq).upper()
				outfile.write('>'+seq.id+'\n'+seq.seq+'\n')
		infile.close()
		outfile.close()
		args['fna_dir'] = args['out_dir']+'/temp/genomes'

	if 'rep_fna' in args and args['rep_fna'] is not None:
		infile = open(args['rep_fna'])
		outfile = open(args['out_dir']+'/temp/'+os.path.basename(args['rep_fna']), 'w')
		for seq in Bio.SeqIO.parse(infile, 'fasta'):
			seq.id = seq.id.replace('-', '_')
			seq.seq = str(seq.seq).upper()
			outfile.write('>'+seq.id+'\n'+seq.seq+'\n')
		infile.close()
		outfile.close()
		args['rep_fna'] = args['out_dir']+'/temp/'+os.path.basename(args['rep_fna'])

def locate_fpaths(args, in_dir, rep_fna=None, subset_list=None):
	subset_map = dict()

	for f in os.listdir(in_dir):
		subset_map[f] = 1

	if subset_list is not None:
		subset_map = dict()
		with open(subset_list, 'r') as fh:
			for ln in fh:
				subset_map[ln.rstrip()] = 1

	args["subset_map"] = subset_map

	ref_path = ""
	fpaths = []

	# Using the largest genome file in direcory for reference intead of randomly selecting anyone
	lg_fpath = ""
	cur_size = 0
	for f in os.listdir(in_dir):
		if f in subset_map:
			fpath = in_dir.rstrip('/')+'/'+f
			if os.path.isfile(fpath):
				fstats = os.stat(fpath)
				fpaths.append(fpath)
				if fstats.st_size >= cur_size:
					cur_size = fstats.st_size
					lg_fpath = fpath
		else:
			sys.stderr.write("skip {}\n".format(f))

	if rep_fna is not None: # Using speficied reference genome
		ref_path = rep_fna
	else:
		ref_path = lg_fpath

	args['rep_fna'] = ref_path
	args['fna_paths'] = fpaths

def detect_single_chrom(ref_path):
	single_chrom = True
	chrom_cnt = 0
	with open(ref_path, 'r') as fh:
		for line in fh:
			if line[0] == '>':
				chrom_cnt = chrom_cnt + 1

				if chrom_cnt == 1:
					pass
				else:
					single_chrom = False
					break

	return single_chrom

def register_run_id(args, in_dir):
	args['run_id'] = in_dir.rstrip('/').split('/')[-1]

	return args['run_id']

def register_msa_id(args, ref_path, fpaths):
	order_names = []

	for fpath in fpaths:
		order_names.append(fpath.rstrip('/').split('/')[-1])

	order_names.append(ref_path.rstrip('/').split('/')[-1])

	in_string = "".join(order_names)
	args['msa_id'] = hashlib.md5(in_string.encode()).hexdigest()

	return args['msa_id']

def auto_min_pid_by_delta(coords_path, idt_delta):
	min_pid_by_delta = 0

	# fields = [('s1',int),('e1',int),
	#		  ('s2',int),('e2',int),
	#		  ('len1',int),('len2',int),
	#		  ('pid',float),
	#		  ('c1',str),('c2',str)]

	pids = []
	with open(coords_path) as f:
		for i in range(5):
			next(f)
		for l in f:
			values = l.replace(' | ', ' ').split()
			pid = float(values[6])
			pids.append(pid)

	avg_pid = sum(pids)/len(pids)

	min_pid_by_delta = avg_pid * (1 - idt_delta)

	return min_pid_by_delta

def run_mummer4_single(fpath, genome_id, ref_fpath, rep_id, out_dir, skip_align, min_pid, min_aln_len, max_pid_delta):
	print("    %s - %s" % (rep_id, genome_id))

	try: os.makedirs(out_dir)
	except: pass

	log = open(out_dir+'/log','w')

	if skip_align is True and os.path.isfile("%s/%s.delta" % (out_dir, genome_id)):
		log.write('nucmer alignment was skipped\n')
		print('    nucmer alignment skipped\n')
	else:
		command = "nucmer %s " % ref_fpath
		command += "%s " % fpath
		command += "--prefix %s/%s " % (out_dir, genome_id)
		out, err = run_command(command)
		log.write(str(out)+'\n'+str(err))

	command = "delta-filter -q -r "
	command += "-i %s " % str(min_pid)
	command += "-l %s " % str(min_aln_len)
	command += "%s/%s.delta " % (out_dir, genome_id)
	command += "> %s/%s.filter.delta.1" % (out_dir, genome_id)
	out, err = run_command(command)
	log.write(str(out)+'\n'+str(err))

	command = "show-coords "
	command += "%s/%s.filter.delta.1 " % (out_dir, genome_id)
	command += "> %s/%s" % (out_dir, 'coords.tmp')
	out, err = run_command(command)
	log.write(str(out)+'\n'+str(err))

	coords_path = "{}/{}".format(out_dir, 'coords.tmp')
	min_pid_by_delta = auto_min_pid_by_delta(coords_path, max_pid_delta)

	command = "delta-filter -q -r "
	command += "-i %s " % str(min_pid_by_delta)
	command += "-l %s " % str(min_aln_len)
	command += "%s/%s.delta " % (out_dir, genome_id)
	command += "> %s/%s.filter.delta" % (out_dir, genome_id)
	out, err = run_command(command)

	for utility in ['coords', 'snps', 'diff']:
		command = "show-%s " % utility
		command += "%s/%s.filter.delta " % (out_dir, genome_id)
		command += "> %s/%s" % (out_dir, utility)
		out, err = run_command(command)
		log.write(str(out)+'\n'+str(err))


def run_mummer4(args):
	ref_fpath = args['rep_fna']
	if 'tag_ref' in args:
		ref_fpath = args['tag_ref']

	fpaths = args['fna_paths']
	if 'tag_genome_paths' in args:
		fpaths = args['tag_genome_paths']

	register_run_id(args, args['fna_dir'])
	register_msa_id(args, ref_fpath, fpaths)

	print("reference genome path: %s" % ref_fpath)

	args['mummer4_dir'] = args['out_dir']+'/temp/mummer4/'+args['run_id']
	try: os.makedirs(args['mummer4_dir'])
	except: pass

	shutil.copy(ref_fpath, os.path.join(args['mummer4_dir'], 'reference.fna'))

	arg_list = []
	rep_id = ref_fpath.split('/')[-1].replace('.fna', '')

	print("[paired alignment]: start")
	for fpath in fpaths:
		genome_id = fpath.split('/')[-1].replace('.fna', '')
		out_dir = '%s/aln/%s' % (args['mummer4_dir'], genome_id)

		arg_list.append([fpath, genome_id, ref_fpath, rep_id, out_dir, args['skip_align'], args['min_pid'], args['min_aln_len'], args['max_pid_delta']])

	print("[paired alignment]: done")

	parallel(run_mummer4_single, arg_list, args['threads'])

	msa_path = gen_msa.build_msa(indir=args['mummer4_dir'], overwrite=True, subset=args["subset_map"])

	shutil.copy(os.path.join(args['mummer4_dir'], 'reference.fna'), args['out_dir'])

	args['msa_path'] = msa_path
	args['msa_type'] = 'xmfa-mummer4'

def run_mash_scketch(args):
	ref_fpath = args['rep_fna']
	fpaths = args['fna_paths']

	register_run_id(args, args['fna_dir'])
	register_msa_id(args, ref_fpath, fpaths)

	print("reference genome path: %s" % ref_fpath)

	args['mash_dir'] = args['out_dir']+'/temp/mash/'+args['run_id']

	try: os.makedirs(args['mash_dir'])
	except: pass

	args['fna_list_path'] = args['mash_dir'] + '/in_fna.list'

	with open(args['fna_list_path'], 'w') as fh:
		for fpath in fpaths:
			fh.write("{}\n".format(fpath))
	
	print("[building mash sketch]: start")

	command = "mash sketch "
	command += "-k %s " % str(args['sketch_k'])
	command += "-s %s " % str(args['sketch_size'])
	command += "-p %s " % str(args['threads'])
	command += "-o %s " % (args['mash_dir']+'/mash_sketch')
	command += "-l %s " % args['fna_list_path']

	out, err = run_command(command)
	sys.stderr.write(str(out)+'\n'+str(err))

	args['mash_sketch_path'] = args['mash_dir']+'/mash_sketch.msh'

def run_mash_dist(args):
	sketch_path = args['mash_sketch_path']

	assert os.path.exists(sketch_path)

	args['mash_dist_path'] = args['mash_dir'] + '/mash_dist.tsv'

	print("[calculating mash distance]: start")

	command = "mash dist "
	command += "-p %s " % str(args['threads'])
	command += "%s %s " % (sketch_path, sketch_path)
	command += "> %s " % args['mash_dist_path'] 

	out, err = run_command(command)
	sys.stderr.write(str(out)+'\n'+str(err))

def do_precut(args):
	dist_path = args['mash_dist_path']

	assert os.path.exists(dist_path)

	args['cut_dist_path'] = args['mash_dir'] + '/mash_dist.cut.tsv'

	print("[cut mash distance: {}]: start".format(str(args['precut'])))

	command = "awk '$3 < %s' " % str(args['precut'])
	command += "%s " % dist_path
	command += "> %s " % args['cut_dist_path']
	
	out, err = run_command(command)
	sys.stderr.write(str(out)+'\n'+str(err))

def id_clusters(args):
	run_mash_scketch(args)

	run_mash_dist(args)

	do_precut(args)

	dist_path = args['cut_dist_path']

	assert os.path.exists(dist_path)

	s_cut = args['start_cutoff']
	e_cut = args['end_cutoff']
	r_fac = args['range_factor']

	total_n = len(args['fna_paths'])

	maf = args['snp_freq']

	critical_n = math.ceil(1 / maf)
	
	optimal_clusters, optimal_d, optimal_n = id_genome_clusters.build_genome_blocks(dist_path, total_n, critical_n, s_cut, e_cut, r_fac)

	clust_genomes = dict()
	tag_genomes = []
	for cluster in optimal_clusters:
		tag_genomes.append(cluster.tag_genome)
		for genome in cluster.genomes: 
			clust_genomes[genome] = 1

	
	for fpath in args['fna_paths']:
		if fpath not in clust_genomes:
			tag_genomes.append(fpath)
	
	args['tag_genome_paths'] = tag_genomes

def id_tag_ref(args):
	assert 'mash_dist_path' in args and os.path.exists(args['mash_dist_path'])
	assert 'tag_genome_paths' in args and len(args['tag_genome_paths']) > 1

	dist_path = args['mash_dist_path']
	tag_paths = args['tag_genome_paths']

	centroid = id_centroid.identify(tag_paths, dist_path)
	
	print(centroid)

	args['tag_ref'] = centroid



def main():
	args = parse_args()
	cmdl_str = ' '.join(sys.argv[1:])

	try: os.makedirs(args['out_dir'])
	except: pass

	skip_centroid = False
	if args['rep_fna'] is not None:
		skip_centroid = True

	if args['data_type'] in ['genomes']:
		locate_fpaths(args, args['fna_dir'], args['rep_fna'], args['subset_list'])

	if args['data_type'] in ['genomes']:
		if args["has_completeness"]:
			if args["completeness"]:
				args["min_prev"] = (1 - float(args["missing_ratio"])) * float(args["completeness"])
			elif args["completeness_list"]:
				completeness_map = {}
				with open(args["completeness_list"], 'w') as fh:
					for line in fh:
						items = line.rstrip('').split('\t')
						completeness_map[items[0]] = float(items[1])
				
				ref_fpath = args['rep_fna']
				fpaths = args['fna_paths']
				
				completenesses = []

				for fpath in fpaths:
					fname = fpath.rstrip('/').split('/')[-1]
					if fname in completeness_map:
						completenesses.append(completeness_map[fname])
					else:
						sys.exit("missing completeness: {}".format(fpath))

				avg_completeness = sum(completenesses)/len(completenesses)
				args["min_prev"] = (1 - float(args["missing_ratio"])) * avg_completeness
			else:
				print("useless option --has-completeness")

	if args['data_type'] in ['genomes']:
		if not args['keep_redundancy']:
			id_clusters(args)

		if not skip_centroid:
			id_tag_ref(args)

	# >>> 1. Generate multiple-genome-alignment or pileups

	# data type is genomes: use parsnp to perform multiple genome alignment
	start = time.time()
	if args['data_type'] == 'genomes':
		print("Running mummer4; start")
		run_mummer4(args)
		#args['mummer4_dir'] = '/Users/jasonshi/Documents/zjshi_github/snpMLST/unit_test_raw/snps_from_genomes/Borrelia_burgdorferi_56121/temp/mummer4/54d64396-732c-42b0-8e88-3de63e8a665e/msa.fna'
		# msa_path = gen_msa.build_msa(indir=args['mummer4_dir'], max_genomes=1280)
		# args['msa_path'] = '/Users/jasonshi/Documents/zjshi_github/snpMLST/unit_test_raw/snps_from_genomes/Borrelia_burgdorferi_56121/temp/mummer4/54d64396-732c-42b0-8e88-3de63e8a665e/msa.fa'
		# args['msa_type'] = 'xmfa-mummer4'
		print("Running mummer4; done!")
	print("Elapsed time: {}".format(time.time()-start))


	# >>> 2. Parse multiple-genome-alignment or pileup and call SNPs

	# fetch generator to parse msa columns or mpileup sites
	start = time.time()
	print("Fetching file-type-specific parser; start")
	if args['data_type'] in ['genomes', 'msa']:
		from align_io import msa
		if args['iter']:
			site_assembly = msa.iter_parse(args['msa_path'], args['msa_type'], args['max_samples'])
		else:
			site_assembly = msa.monolithic_parse(args['msa_path'], args['msa_type'], args['max_samples'])

	print("Fetching file-type-specific parser; done")
	print("Elapsed time: {}".format(time.time()-start))


	# id core-genome coords and snps
	start = time.time()
	print("Identifying core-snps; start")
	print("max sites: {}".format(args['max_sites']))
	print("min prevalence: {}".format(args['min_prev']))
	print("min MAF: {}".format(args['snp_freq']))

	if args['iter']:
		align_assembs = align_assembly.call_snps_iter(site_assembly, args['max_sites'], args['min_prev'], args['snp_freq'])
	else:
		align_assembs = align_assembly.call_snps(site_assembly, args['max_sites'], args['min_prev'], args['snp_freq'])
	print("Identifying core-snps; done")
	print("Elapsed time: {}".format(time.time()-start))

	# sys.exit()

	single_chrom_rep = False

	if args['iter'] is True and args['rep_fna'] is not None:
		single_chrom_rep = detect_single_chrom(args['rep_fna'])

	# write output files
	start = time.time()
	print("Writing snps to VCF; start")
	if args['iter']:
		header_ready = False
		coords_buffer = []
		for align_assemb in align_assembs:
			if len(align_assemb.snps) > 0:
				if not header_ready:
					vcf_io.write_coords_header(coords_buffer, args['out_dir'])
					vcf_io.write_vcf_header(align_assemb.snps, args['out_dir'], cmdl_str)
					header_ready = True

				# vcf_io.write_genome(core_genome.consensus_genome, args['out_dir'])
				coords_buffer = coords_buffer + align_assemb.coords
				vcf_io.write_vcf(align_assemb.snps, args['out_dir'], single_chrom_rep)

		vcf_io.write_coords(vcf_io.merge_coords(coords_buffer), args['out_dir'])
		# vcf_io.write_coords(coords_buffer, args['out_dir'])
	else:
		vcf_io.write_coords_header(align_assembs.coords, args['out_dir'])
		vcf_io.write_vcf_header(align_assembs.snps, args['out_dir'], cmdl_str)
		vcf_io.write_coords(align_assembs.coords, args['out_dir'])
		# vcf_io.write_genome(core_genome.consensus_genome, args['out_dir'])
		vcf_io.write_vcf(align_assembs.snps, args['out_dir'])
	print("Writing snps to VCF; done!")
	print("Elapsed time: {}".format(time.time()-start))


if __name__ == "__main__":
	main()
